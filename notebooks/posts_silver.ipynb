{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "330a20bb-f1ef-4f4a-8f3b-919a9c67fc0b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Read Table"
    }
   },
   "outputs": [],
   "source": [
    "raw_posts_df = spark.read.table('default.raw_posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "352dad20-68f4-4a60-b05a-a1c68263e771",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Transformations"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, MapType, IntegerType, StringType\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "\n",
    "def split_tag_into_array(df: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        df\n",
    "        .withColumn(\"TagsArray\", F.filter(F.split(F.col(\"Tags\"), r'\\|'), lambda x: x != \"\"))\n",
    "        .drop(\"Tags\")\n",
    "    )\n",
    "\n",
    "def rename_columns(df: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        df.withColumnRenamed(\"Id\", \"PostId\")\n",
    "    )\n",
    "\n",
    "def map_post_type(df: DataFrame) -> DataFrame:\n",
    "    map_data = [\n",
    "        (1, \"Question\"),\n",
    "        (2, \"Answer\"),\n",
    "        (3, \"Orphaned tag wiki\"),\n",
    "        (4, \"Tag wiki excerpt\"),\n",
    "        (5, \"Tag wiki\"),\n",
    "        (6, \"Moderator nomination\"),\n",
    "        (7, \"Wiki placeholder\"),\n",
    "        (8, \"Privilege wiki\"),\n",
    "        (9, \"Article\"),\n",
    "        (10, \"HelpArticle\"),\n",
    "        (12, \"Collection\"),\n",
    "        (13, \"ModeratorQuestionnaireResponse\"),\n",
    "        (14, \"Announcement\"),\n",
    "        (15, \"CollectiveDiscussion\"),\n",
    "        (17, \"CollectiveCollection\")\n",
    "    ]\n",
    "\n",
    "    map_schema = StructType([\n",
    "        StructField(\"PostTypeId\", IntegerType(), False),\n",
    "        StructField(\"PostType\", StringType(), False)\n",
    "    ])\n",
    "\n",
    "    map_df = spark.createDataFrame(map_data, schema=map_schema)\n",
    "\n",
    "    return df.join(\n",
    "        F.broadcast(map_df),\n",
    "        df[\"PostTypeId\"] == map_df[\"PostTypeId\"],\n",
    "        \"left\"\n",
    "    ).drop(map_df[\"PostTypeId\"])\n",
    "\n",
    "stg_posts_df = (\n",
    "    raw_posts_df\n",
    "    .transform(split_tag_into_array)\n",
    "    .transform(rename_columns)\n",
    "    .transform(map_post_type)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3f9bf53-0cec-4872-9fce-8751ed02218c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Incremental Upsert"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "from delta.tables import DeltaTable\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def incremntal_upsert(df: DataFrame, dest_table: str, unique_key: str, updated_at: str, full_refresh: bool = False) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Performs incremental upserts using updated_at as the cursor value and unique key\n",
    "    \"\"\"\n",
    "\n",
    "    if not spark.catalog.tableExists(dest_table) or full_refresh:\n",
    "        (\n",
    "            df.write.format(\"delta\")\n",
    "                    .mode(\"overwrite\")\n",
    "                    .option(\"overwriteSchema\", \"true\")\n",
    "                    .saveAsTable(dest_table)\n",
    "         )\n",
    "    else:\n",
    "        # Get the latest value of updated_at\n",
    "        last_max = (\n",
    "                    spark.table(dest_table)\n",
    "                    .agg(F.max(updated_at).alias(\"max_ts\"))\n",
    "                    .collect()[0][\"max_ts\"]\n",
    "                )\n",
    "\n",
    "        # Filter the new data to only include rows with updated_at > latest_update\n",
    "        new_data = df.filter(F.col(updated_at) > last_max)\n",
    "\n",
    "        # Upsert the new data\n",
    "        if new_data.limit(1).count() > 0:\n",
    "            delta_table = DeltaTable.forName(spark, dest_table)\n",
    "            (\n",
    "                delta_table.alias(\"t\")\n",
    "                    .merge(source=new_data.alias(\"s\"),\n",
    "                        condition=f\"t.{unique_key} = s.{unique_key}\")\n",
    "                    .whenMatchedUpdateAll()\n",
    "                    .whenNotMatchedInsertAll()\n",
    "                    .execute()\n",
    "            )\n",
    "    return df\n",
    "\n",
    "dest_table = \"default.stg_posts\"\n",
    "stg_posts_df = incremntal_upsert(stg_posts_df, dest_table, \"PostId\",\"CreationDate\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "posts_silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
